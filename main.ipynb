{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas_profiling только с версией из гит и на версии питона <3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pydantic\n",
    "pydantic.__version__\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title='Spaceship Titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Deck']=df[\"Cabin\"].apply(lambda x: str(x).split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cabin(x):\n",
    "  if len(str(x).split('/')) < 3:\n",
    "    return ['Missing', 'Missing', \"Missing\"]\n",
    "  else:   \n",
    "    return str(x).split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a predprocessing function to transform database\n",
    "def preprocessing(df):\n",
    "    # Fill missing values in homeplanet with missing \n",
    "    df['HomePlanet'].fillna('Missing', inplace=True)\n",
    "    # Cryosleep - highly correlated - drop na rows\n",
    "    df['CryoSleep'].fillna('Missing', inplace=True)\n",
    "    # Cabin preprocessing - extract Deck and Side \n",
    "    df['TempCabin'] = df['Cabin'].apply(lambda x: split_cabin(x))\n",
    "    df['Deck'] = df['TempCabin'].apply(lambda x: x[0])\n",
    "    df['Side'] = df['TempCabin'].apply(lambda x: x[2])\n",
    "    df.drop(['TempCabin', 'Cabin'], axis=1, inplace=True) \n",
    "    df['Destination'].fillna('Missing', inplace=True)\n",
    "    # Age \n",
    "    df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "    # VIP - drop na rows\n",
    "    df['VIP'].fillna('Missing', inplace=True)\n",
    "    # Monetary spending columns \n",
    "    df['RoomService'].fillna(0, inplace=True)\n",
    "    df['FoodCourt'].fillna(0, inplace=True) \n",
    "    df['ShoppingMall'].fillna(0, inplace=True)\n",
    "    df['Spa'].fillna(0, inplace=True)\n",
    "    df['VRDeck'].fillna(0, inplace=True)\n",
    "    # Drop name due to high cardinality\n",
    "    df.drop('Name', axis=1, inplace=True)\n",
    "    # Drop remaining rows\n",
    "    #df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = df.copy()\n",
    "preprocessing(abt)\n",
    "abt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "- Feature and Target values - X, y\n",
    "- One hot encode any categorical features\n",
    "- Train, holdout split\n",
    "- Train on a bunch of algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abt.drop(['Transported', 'PassengerId'], axis=1)\n",
    "X = pd.get_dummies(X)\n",
    "y = abt['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state=1234)),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=1234))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'rf': {\n",
    "        'randomforestclassifier__n_estimators':[100,200,300]\n",
    "    },\n",
    "    'gb':{\n",
    "        'gradientboostingclassifier__n_estimators':[100,200,300]\n",
    "    } \n",
    "}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank dictionary to hold the models \n",
    "fit_models = {}\n",
    "# Loop through all the algos \n",
    "for algo, pipeline in pipelines.items():\n",
    "  print(f'Training the {algo} model.')\n",
    "  # Create new Grid Search CV Cclass \n",
    "  model = GridSearchCV(pipeline, grid[algo], n_jobs=-1, cv=10)\n",
    "  # Train the model \n",
    "  model.fit(X_train, y_train)\n",
    "  # Store results inside of the dictionary\n",
    "  fit_models[algo] = model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance on Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alg, model in fit_models.items(): \n",
    "  yhat = model.predict(X_test)\n",
    "  accuracy = accuracy_score(y_test, yhat)\n",
    "  precision = precision_score(y_test, yhat)\n",
    "  recall = recall_score(y_test, yhat)\n",
    "  print(f'Metrics for {alg}: accuracy- {accuracy}, recall- {recall}, precision- {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gradientboosted.pkl', 'wb') as f: \n",
    "  pickle.dump(fit_models['gb'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gradientboosted.pkl', 'rb') as f: \n",
    "  reloaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Test CSV Dataset\n",
    "test_df = pd.read_csv('test.csv')\n",
    "# Deep copy\n",
    "abt_test = test_df.copy()\n",
    "# Run through the preocessing pipeline\n",
    "preprocessing(abt_test)\n",
    "# One hot encode categorical variables\n",
    "abt_test = pd.get_dummies(abt_test.drop('PassengerId', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = fit_models['gb'].predict(abt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame([test_df['PassengerId'], yhat_test]).T\n",
    "submission.columns = ['PassengerID', 'Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index=False, чтобы не писался номер слева таблицы\n",
    "submission.to_csv('kaggle_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
